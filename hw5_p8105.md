hw5_p8105
================
2024-11-11

## Problem 2

**2A. Design the elements**

``` r
#mu_values = 0:6
#for i in 1:length(mu_mean)
#define the length in the function not in the loop

#n = 30
#SD = 5
#mean = 0:6
#a = rnorm(30, mean = 0, sd = 5)
#include a in the function so it become repeated sampling, the function use t.test directly

ttest = function(n, mean, sd) {
  df1 = tibble(
    x = rnorm(n = 30, mean, sd = 5))
  ttest_result = t.test(df1)
  broom::tidy(ttest_result)
}

#writing loop
#use tidy as part of the output immediately to create a tibble
mean_0 = vector("list", length = 5000)

for (i in 1:5000) {
  mean_0[[i]] = ttest(mean = 0)
}

#tidy dataframe result
result0 = bind_rows(mean_0)
```

**2B. Loop for mean 1-6**

``` r
#repeat the above for mean = 1-6, use map is easier
#y should not be part of the expand_grid. we shouldn't have 30 rows, we should have 5000 rows of random sampling containing 30 numbers.
mean_res = 
  expand_grid(
    true_mean = c(0, 1, 2, 3, 4, 5, 6),
    iter = 1:5000
  ) %>% 
  mutate(ttest_result = map(true_mean, ~ttest(mean = true_mean))) %>% 
  unnest(ttest_result)
```

**2C. Make plot for effect size**

``` r
mean_res %>% 
  mutate(
    estimate = as.numeric(estimate)
  ) %>% 
  select(true_mean, iter, estimate, p.value) %>% 
  group_by(true_mean) %>% 
  mutate(reject_null = p.value < 0.05) %>% 
  summarize(power = mean(reject_null)) %>% 
  ggplot(aes(x = true_mean, y = power)) +
  geom_point() +
  geom_smooth(se = FALSE)
```

    ## `geom_smooth()` using method = 'loess' and formula = 'y ~ x'

![](hw5_p8105_files/figure-gfm/effect%20size-1.png)<!-- -->

``` r
#the code works, but the graph is weird bc the statistics is wrong
```

**Describe the association between effect size and power**

Something is wrong with the statistics code, which I don’t know why
because I’m not good with statistics. But in general, as the
hypothesized true population mean increased, so does the difference with
the sample mean. As effect size increase, power will also increase as we
can detect difference and thus reject the null.

**2D. Make plot for mu hat true mu**

``` r
#plot 2
plot2 = mean_res %>% 
  group_by(true_mean) %>% 
  summarize(mean(estimate)) %>% 
  rename(mu_hat_2 = `mean(estimate)`) 

ggplot(plot2, aes(x = true_mean, y = mu_hat_2)) +
  geom_point() +
  geom_smooth(se = FALSE)
```

    ## `geom_smooth()` using method = 'loess' and formula = 'y ~ x'

![](hw5_p8105_files/figure-gfm/mu-1.png)<!-- -->

``` r
#plot 3
plot3 = mean_res %>% 
  filter(p.value < 0.05) %>% 
  group_by(true_mean) %>% 
  summarize(mean(estimate)) %>% 
  rename(mu_hat_2f = `mean(estimate)`)

ggplot(plot3, aes(x = true_mean, y = mu_hat_2f)) +
  geom_point() +
  geom_smooth(se = FALSE)
```

    ## `geom_smooth()` using method = 'loess' and formula = 'y ~ x'

![](hw5_p8105_files/figure-gfm/mu-2.png)<!-- -->

**Is the sample average of mu hat across tests for which the null is
rejected approximately equal to the true value of mean? why or why
not?** Again, the plot is weird because something is wrong, but
theoretically, the average of mu hat from null is rejected should not be
the same to the true value of the mean because the null rejected,
meaning mu != mu hat.

## Problem 3

**3A**

``` r
#export the data
homicide = read.csv("./data/homicide-data.csv")
```

**Describe the raw data** The data contains 52179 observations. The
variables are uid, reported_date, victim_last, victim_first,
victim_race, victim_age, victim_sex, city, state, lat, lon, disposition,
which includes date of the crime (in YYYYMMDD format), victim’s name and
demographics, location of homicide, and the outcome of the
investigation.

Create a city_state variable (e.g. “Baltimore, MD”) and then summarize
within cities to obtain the total number of homicides and the number of
unsolved homicides (those for which the disposition is “Closed without
arrest” or “Open/No arrest”).

``` r
homicide_clean = homicide %>% 
  unite(city_state, c(city, state), sep = ", ") %>% 
  group_by(city_state) %>%
  summarize(
    total = n(), 
    unsolved = sum(disposition != "Closed by arrest"))
```

Total number of homicides and unsolved homicides across all city states
are 52179 and 26505, respectively.

**3B.** For the city of Baltimore, MD, use the prop.test function to
estimate the proportion of homicides that are unsolved; save the output
of prop.test as an R object, apply the broom::tidy to this object and
pull the estimated proportion and confidence intervals from the
resulting tidy dataframe.

``` r
library(broom)

#prop test for baltimore. x = unsolved, n = total homicide, the rest use default setting
homicide_clean %>% 
  filter(city_state == "Baltimore, MD") %>% 
  mutate(
    prop_test_result = map2(unsolved, total, ~ prop.test(x = .x, n = .y))
  ) %>% 
  mutate(
    tidy_result = map(prop_test_result, broom::tidy)
  ) %>% 
  unnest(tidy_result)
```

    ## # A tibble: 1 × 12
    ##   city_state    total unsolved prop_test_result estimate statistic  p.value
    ##   <chr>         <int>    <int> <list>              <dbl>     <dbl>    <dbl>
    ## 1 Baltimore, MD  2827     1825 <htest>             0.646      239. 6.46e-54
    ## # ℹ 5 more variables: parameter <int>, conf.low <dbl>, conf.high <dbl>,
    ## #   method <chr>, alternative <chr>

The estimated proportion for unsolved cases are — with confidence
interval between —

**3C** Now run prop.test for each of the cities in your dataset, and
extract both the proportion of unsolved homicides and the confidence
interval for each. Do this within a “tidy” pipeline, making use of
purrr::map, purrr::map2, list columns and unnest as necessary to create
a tidy dataframe with estimated proportions and CIs for each city.

``` r
homicide_plot = homicide_clean %>% 
  mutate(
    prop_test_result = map2(unsolved, total, ~ prop.test(x = .x, n = .y))
  ) %>% 
  mutate(
    tidy_result = map(prop_test_result, broom::tidy)
  ) %>% 
  unnest(tidy_result)
```

    ## Warning: There was 1 warning in `mutate()`.
    ## ℹ In argument: `prop_test_result = map2(unsolved, total, ~prop.test(x = .x, n =
    ##   .y))`.
    ## Caused by warning in `prop.test()`:
    ## ! Chi-squared approximation may be incorrect

Create a plot that shows the estimates and CIs for each city – check out
geom_errorbar for a way to add error bars based on the upper and lower
limits. Organize cities according to the proportion of unsolved
homicides

``` r
homicide_plot %>% 
  select(city_state, estimate, conf.low, conf.high) %>% 
  arrange(desc(estimate)) %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust  =1))
```

![](hw5_p8105_files/figure-gfm/plot%20homicide-1.png)<!-- -->
