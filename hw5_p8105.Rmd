---
title: "hw5_p8105"
output: github_document
date: "2024-11-11"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(broom)
library(ggridges)
set.seed(1)
```

## Problem 1
Not graded

```{r birthday}
```

## Problem 2
```{r power}
#test df
df1 = tibble(
  x = rnorm(30, mean = 0, sd = 5))

means = seq(0, 6, by = 1)

#test function
#need to add the return so the console display the result

df_sample = function(n, mean, sd) {
  data_1 = tibble(
    x = rnorm(n, mean, sd))
  data_result = t.test(data_1)
  
  return(data_result)
}

#you can run the function with the exact same n and sd and only change the mean
df_sample(30, 0, 5)
df_sample(30, 1, 5)



#function: the dataset is part of the function because it's needed for repeated sampling
#the function is t.test directly
a_sample = function(a) {
  a_data = tibble(
    a = rnorm(30, mean = 0, sd = 5))
  a_result = t.test(a_data)
}

#writing loop
#use tidy as part of the output immediately to create a tibble
mean_0 = vector("list", length = 5000)

for (i in 1:5000) {
  mean_0[[i]] = tidy(a_sample())
}

#tidy dataframe result
result0 = bind_rows(mean_0) %>% 
  mutate(mean = "0") %>% 
  select(mean, estimate, p.value)

#repeat the above for mean = 1-6
#mean = 1
mean_1 = vector("list", length = 5000)

for (i in 1:5000) {
  mean_1[[i]] = tidy(a_sample(a(mean = 1)))
}

result1 = bind_rows(mean_1) %>% 
  mutate(mean = "1") %>% 
  select(mean, estimate, p.value)

#mean = 2
mean_2 = vector("list", length = 5000)

for (i in 1:5000) {
  mean_2[[i]] = tidy(a_sample(a(mean = 2)))
}

result2 = bind_rows(mean_2) %>% 
  mutate(mean = "2") %>% 
  select(mean, estimate, p.value)

#mean = 3
mean_3 = vector("list", length = 5000)

for (i in 1:5000) {
  mean_3[[i]] = tidy(a_sample(a(mean = 3)))
}

result3 = bind_rows(mean_3)  %>% 
  mutate(mean = "3") %>% 
  select(mean, estimate, p.value)

#mean = 4
mean_4 = vector("list", length = 5000)

for (i in 1:5000) {
  mean_4[[i]] = tidy(a_sample(a(mean = 4)))
}

result4 = bind_rows(mean_4) %>% 
  mutate(mean = "4") %>% 
  select(mean, estimate, p.value)

#mean = 5
mean_5 = vector("list", length = 5000)

for (i in 1:5000) {
  mean_5[[i]] = tidy(a_sample(a(mean = 5)))
}

result5 = bind_rows(mean_5)  %>% 
  mutate(mean = "5") %>% 
  select(mean, estimate, p.value)

#mean = 6
mean_6 = vector("list", length = 5000)

for (i in 1:5000) {
  mean_6[[i]] = tidy(a_sample(a(mean = 6)))
}

result6 = bind_rows(mean_6)  %>% 
  mutate(mean = "6") %>% 
  select(mean, estimate, p.value)

#How to include # of iterations and input mean in the tibble? mutate
#technically the df making (bind rows, mutate, and select) part can be included in a loop

#write a function to make the plot

#1st plot: Y = proportion of null rejected (p<0.05); x = the mean
#do i bind rows then filter, OR filter first then bind rows?

#bind rows first (make a master df)

masterdf = 
  bind_rows(result0, result1, result2, result3, result4, result5, result6) %>% 
  mutate(mean = as.numeric(mean))

#plot1
plot1 = masterdf %>% 
  filter(p.value < 0.05) %>% 
  count(mean) %>% 
  mutate(proportion = n/5000) 

#ggplot weirdly convert my df into list if i pipe directly, so i need to use the print function to display the plot. that's why i don't use piping. 
ggplot(plot1, aes(x = mean, y = proportion)) +
  geom_point()

#describe the association between effect size and power
#In this simulation, out of 5000 only around ~250ish of all mean rejected the null hypothesis.
#This means that the effect size in majority of the simulations is small
#Because it's small, the power is small


#plot 2
plot2 = masterdf %>% 
  group_by(mean) %>% 
  summarize(mean(estimate)) %>% 
  rename(mu_hat_2 = `mean(estimate)`)

ggplot(plot2, aes(x = mean, y = mu_hat_2)) +
  geom_point() +
  geom_smooth(se = FALSE)

#plot 3
plot3 = masterdf %>% 
  filter(p.value < 0.05) %>% 
  group_by(mean) %>% 
  summarize(mean(estimate)) %>% 
  rename(mu_hat_2f = `mean(estimate)`)

ggplot(plot3, aes(x = mean, y = mu_hat_2f)) +
  geom_point() +
  geom_smooth(se = FALSE)

#the plot feels weird because there are - values. should i specify the data to be + only?

#how to overlap plot?

#-----
comp_plot = full_join(plot2, plot3)

#is the sample average of mu hat across tests for which the null is rejected approximately equal to the true value of mean? why or why not? 
#no. actually the data from mu_hat_2 has the closer value to the true mean compared to the filtered one. this is because they have bigger sample size (5000 vs 250 ish)

```

## Problem 3
```{r homicide}
#export the data
homicide = read.csv("./data/homicide-data.csv")
```

Describe the raw data: The data contains `r nrow(homicide)` observations. The variables are `r names(homicide)`, which includes date of the crime (in YYYYMMDD format), victim's name and demographics, location of homicide, and the outcome of the investigation. 

Create a city_state variable (e.g. “Baltimore, MD”) and then summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).
```{r homicide2}
homicide %>% 
  unite(city_state, c(city, state), sep = ", ") %>% 
  group_by(city_state) %>% 
  summarize(count = n())
```

*Am I supposed to merge homicide2 and homicide3? 

```{r homicide3}
homicide %>% 
  unite(city_state, c(city, state), sep = ", ") %>% 
  group_by(city_state, disposition) %>% 
  filter(disposition != "Closed by arrest") %>% 
  summarize(count = n())
  
```

For the city of Baltimore, MD, use the prop.test function to estimate the proportion of homicides that are unsolved; save the output of prop.test as an R object, apply the broom::tidy to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.
```{r homicide4}
#prop test for baltimore. x = unsolved, n = total homicide, the rest use default setting
md_result = tidy(prop.test(1825, 2827)) %>% 
  mutate(city_state = "baltimore_md")
```

Now run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. Do this within a “tidy” pipeline, making use of purrr::map, purrr::map2, list columns and unnest as necessary to create a tidy dataframe with estimated proportions and CIs for each city.
```{r homicide5}
#now need to make a loop for function in homicide4 to look at all city state
#need to join homicide2 (total) and homicide3 (unsolved) 

#OR

#the function needs to summarize total for each citystate, the count the unsolved one

#the function needs to be able to sample from the df, specifying each city state, x = unsolved, x = total homicide

#where does citystate will be included in the prop.test function?

```


Create a plot that shows the estimates and CIs for each city – check out geom_errorbar for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides


