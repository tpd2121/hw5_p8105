---
title: "hw5_p8105"
output: github_document
date: "2024-11-11"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(broom)
library(ggridges)
set.seed(1)
```

## Problem 2

**2A. Design the elements**
```{r elements}

#mu_values = 0:6
#for i in 1:length(mu_mean)
#define the length in the function not in the loop

#n = 30
#SD = 5
#mean = 0:6
#a = rnorm(30, mean = 0, sd = 5)
#include a in the function so it become repeated sampling, the function use t.test directly

ttest = function(x) {
  df1 = tibble(
    x = rnorm(30, mean = x, sd = 5))
  ttest_result = t.test(df1)
  broom::tidy(ttest_result)
}

#writing loop
#use tidy as part of the output immediately to create a tibble
mean_0 = vector("list", length = 5000)

for (i in 1:5000) {
  mean_0[[i]] = ttest(0)
}

#tidy dataframe result
result0 = bind_rows(mean_0)

```

Option 2: use map
```{r elements2}
y = rnorm(30, mean = 0, sd = 5)
true_mean = c(0, 1)

test_res = 
  expand_grid(
    iter = 1:5,
    y = rnorm(30, mean = true_mean, sd = 5)
  ) %>% 
  mutate(result1 = map(iter, sampling(y))) %>% 
  broom::tidy(ttest_result[[1]])
```

**2B. Loop for mean 1-6**
```{r loop 1-6}
#repeat the above for mean = 1-6, use map is easier
#y should not be part of the expand_grid. we shouldn't have 30 rows, we should have 5000 rows of random sampling containing 30 numbers.
mean_res = 
  expand_grid(
    true_mean = c(0, 1, 2, 3, 4, 5, 6),
    iter = 1:5000
  ) %>% 
  mutate(ttest_result = map(true_mean, ~ttest(true_mean))) %>% 
  unnest(ttest_result)
```

**2C. Make plot for effect size**
```{r effect size}
mean_res %>% 
  mutate(
    estimate = as.numeric(estimate)
  ) %>% 
  select(true_mean, iter, estimate, p.value) %>% 
  group_by(true_mean) %>% 
  mutate(reject_null = p.value < 0.05) %>% 
  summarize(power = mean(reject_null)) %>% 
  ggplot(aes(x = true_mean, y = power)) +
  geom_point() +
  geom_smooth(se = FALSE)

#this is what i used earlier in lieu of reject null and power. result is similar
  filter(p.value < 0.05) %>% 
  summarize(count = n()) %>% 
  mutate(proportion = count/10) %>% 
  ggplot(aes(x = true_mean, y = proportion)) +
  geom_point() +
  geom_smooth(se = FALSE)

#the code works, but the graph is weird bc the statistics is wrong
```

**Describe the association between effect size and power**

Something is wrong with the statistics code, which I don't know why because I'm not good with statistics. But in general, as the hypothesized true population mean increased, so does the difference with the sample mean. As effect size increase, power will also increase as we can detect difference and thus reject the null. 

**2D. Make plot for mu hat true mu**
```{r mu}
#plot 2
plot2 = mean_res %>% 
  group_by(true_mean) %>% 
  summarize(mean(estimate)) %>% 
  rename(mu_hat_2 = `mean(estimate)`) 

ggplot(plot2, aes(x = true_mean, y = mu_hat_2)) +
  geom_point() +
  geom_smooth(se = FALSE)

#plot 3
plot3 = mean_res %>% 
  filter(p.value < 0.05) %>% 
  group_by(true_mean) %>% 
  summarize(mean(estimate)) %>% 
  rename(mu_hat_2f = `mean(estimate)`)

ggplot(plot3, aes(x = true_mean, y = mu_hat_2f)) +
  geom_point() +
  geom_smooth(se = FALSE)

#is the sample average of mu hat across tests for which the null is rejected approximately equal to the true value of mean? why or why not? 
#no. actually the data from mu_hat_2 has the closer value to the true mean compared to the filtered one. this is because they have bigger sample size (5000 vs 250 ish)

```

**Is the sample average of mu hat across tests for which the null is rejected approximately equal to the true value of mean? why or why not?**
Again, the plot is weird because something is wrong, but theoretically, the average of mu hat from null is rejected should not be the same to the true value of the mean because the null rejected, meaning mu != mu hat. 

## Problem 3

**3A**
```{r homicide}
#export the data
homicide = read.csv("./data/homicide-data.csv")
```

**Describe the raw data** 
The data contains `r nrow(homicide)` observations. The variables are `r names(homicide)`, which includes date of the crime (in YYYYMMDD format), victim's name and demographics, location of homicide, and the outcome of the investigation. 

Create a city_state variable (e.g. “Baltimore, MD”) and then summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).

```{r hom test}
homicide_clean = homicide %>% 
  unite(city_state, c(city, state), sep = ", ") %>% 
  group_by(city_state) %>%
  summarize(
    total = n(), 
    unsolved = sum(disposition != "Closed by arrest"))
```

**3B.** 
For the city of Baltimore, MD, use the prop.test function to estimate the proportion of homicides that are unsolved; save the output of prop.test as an R object, apply the broom::tidy to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.
```{r homicide_md}
#prop test for baltimore. x = unsolved, n = total homicide, the rest use default setting
homicide_clean %>% 
  filter(city_state == "Baltimore, MD") %>% 
  mutate(
    prop_test_result = map(city_state, ~ prop.test(x = unsolved, n = total))
    ) %>% 
  unnest(prop_test_result)
    
#the code works, but can't extract the prop_test_result --> unnest and tidy doesn't work
```

**3C**
Now run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. Do this within a “tidy” pipeline, making use of purrr::map, purrr::map2, list columns and unnest as necessary to create a tidy dataframe with estimated proportions and CIs for each city.
```{r homicide5}
homicide_clean %>% 
  mutate(
    prop_test_result = map(city_state, ~ prop.test(x = unsolved, n = total))
    )

#how to extract?

#------

#previous code
hom_result = homicide %>% 
  unite(city_state, c(city, state), sep = ", ") %>% 
  group_by(city_state) %>%
  summarize(
    total = n(), 
    unsolved = sum(disposition != "Closed by arrest")) %>%
  rowwise() %>%
  mutate(
    test_result = list(prop.test(x = unsolved, n = total)),
    p_value = test_result$p.value,
    estimate = test_result$estimate,
    conf_low = test_result$conf.int[1],
    conf_high = test_result$conf.int[2]
  ) %>%
  unnest(c(p_value, estimate, conf_low, conf_high))

#he wants to use purr so need to make a loop (which is???)

#write the function
homicide_loop = function(u) {
  hom_result = homicide %>% 
  unite(city_state, c(city, state), sep = ", ") %>% 
  group_by(city_state) %>%
  summarize(
    total = n(), 
    unsolved = sum(disposition != "Closed by arrest"))
}

hom_result2 = homicide %>%
  unite(city_state, c(city, state), sep = ", ") %>% 
  group_by(city_state) %>%
  summarize(
    total = n(), 
    unsolved = sum(disposition != "Closed by arrest")) %>% 
  map(hom_result2, prop.test(x = hom_result2$unsolved, n = hom_result2$total)) %>% 
  unnest(c(p_value, estimate, conf_low, conf_high))

# Extract the p-value, estimate, and confidence interval from the prop.test results
df <- df %>%
  mutate(
    p_value = map_dbl(test_result, ~ .x$p.value),
    estimate = map_dbl(test_result, ~ .x$estimate),
    conf_low = map_dbl(test_result, ~ .x$conf.int[1]),
    conf_high = map_dbl(test_result, ~ .x$conf.int[2])
  )
```

Create a plot that shows the estimates and CIs for each city – check out geom_errorbar for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides

```{r plot homicide}

```



