---
title: "hw5_p8105"
output: github_document
date: "2024-11-11"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(broom)
library(ggridges)
set.seed(1)
```

## Problem 2

2A. Design the elements
```{r elements}

#mu_values = 0:6
#for i in 1:length(mu_mean)
#define the length in the function not in the loop

#n = 30
#SD = 5
#mean = 0:6
#a = rnorm(30, mean = 0, sd = 5)
#include a in the function so it become repeated sampling, the function use t.test directly

ttest = function(x) {
  df1 = tibble(
    x = rnorm(30, mean = x, sd = 5))
  ttest_result = t.test(df1)
  broom::tidy(ttest_result)
}

#writing loop
#use tidy as part of the output immediately to create a tibble
mean_0 = vector("list", length = 5000)

for (i in 1:5000) {
  mean_0[[i]] = ttest(0)
}

#tidy dataframe result
result0 = bind_rows(mean_0)

```

Option 2: use map
```{r elements2}
y = rnorm(30, mean = 0, sd = 5)
true_mean = c(0, 1)

test_res = 
  expand_grid(
    iter = 1:5,
    y = rnorm(30, mean = true_mean, sd = 5)
  ) %>% 
  mutate(result1 = map(iter, sampling(y))) %>% 
  broom::tidy(ttest_result[[1]])
```

2B. Loop for mean 1-6
```{r loop 1-6}
#repeat the above for mean = 1-6, use map is easier
#y should not be part of the expand_grid. we shouldn't have 30 rows, we should have 5000 rows of random sampling containing 30 numbers.
mean_res = 
  expand_grid(
    true_mean = c(0, 1, 2, 3, 4, 5, 6),
    iter = 1:10
  ) %>% 
  mutate(ttest_result = map(true_mean, ~ttest(true_mean))) %>% 
  unnest(ttest_result)


#mean = 1
mean_1 = vector("list", length = 5000)

for (i in 1:5000) {
  mean_1[[i]] = tidy(a_sample(a(mean = 1)))
}

result1 = bind_rows(mean_1) %>% 
  mutate(mean = "1") %>% 
  select(mean, estimate, p.value)

#mean = 2
mean_2 = vector("list", length = 5000)

for (i in 1:5000) {
  mean_2[[i]] = tidy(a_sample(a(mean = 2)))
}

result2 = bind_rows(mean_2) %>% 
  mutate(mean = "2") %>% 
  select(mean, estimate, p.value)

#mean = 3
mean_3 = vector("list", length = 5000)

for (i in 1:5000) {
  mean_3[[i]] = tidy(a_sample(a(mean = 3)))
}

result3 = bind_rows(mean_3)  %>% 
  mutate(mean = "3") %>% 
  select(mean, estimate, p.value)

#mean = 4
mean_4 = vector("list", length = 5000)

for (i in 1:5000) {
  mean_4[[i]] = tidy(a_sample(a(mean = 4)))
}

result4 = bind_rows(mean_4) %>% 
  mutate(mean = "4") %>% 
  select(mean, estimate, p.value)

#mean = 5
mean_5 = vector("list", length = 5000)

for (i in 1:5000) {
  mean_5[[i]] = tidy(a_sample(a(mean = 5)))
}

result5 = bind_rows(mean_5)  %>% 
  mutate(mean = "5") %>% 
  select(mean, estimate, p.value)

#mean = 6
mean_6 = vector("list", length = 5000)

for (i in 1:5000) {
  mean_6[[i]] = tidy(a_sample(a(mean = 6)))
}

result6 = bind_rows(mean_6)  %>% 
  mutate(mean = "6") %>% 
  select(mean, estimate, p.value)

#How to include # of iterations and input mean in the tibble? mutate
#technically the df making (bind rows, mutate, and select) part can be included in a loop
```

2C. Make plot for effect size
```{r effect size}
#write a function to make the plot

#1st plot: Y = proportion of null rejected (p<0.05); x = the mean
#do i bind rows then filter, OR filter first then bind rows?

#bind rows first (make a master df)

masterdf = 
  bind_rows(result0, result1, result2, result3, result4, result5, result6) %>% 
  mutate(mean = as.numeric(mean))

#plot1
plot1 = masterdf %>% 
  filter(p.value < 0.05) %>% 
  count(mean) %>% 
  mutate(proportion = n/5000) 

#ggplot weirdly convert my df into list if i pipe directly, so i need to use the print function to display the plot. that's why i don't use piping. 
ggplot(plot1, aes(x = mean, y = proportion)) +
  geom_point()

#describe the association between effect size and power
#In this simulation, out of 5000 only around ~250ish of all mean rejected the null hypothesis.
#This means that the effect size in majority of the simulations is small
#Because it's small, the power is small
```

2D. Make plot for mu hat true mu
```{r mu}

#as you increase your mean, the difference with the sample mean is getting bigger. so the effect size is getting bigger too, and as effect size increase, the power increase. 


#plot 2
plot2 = masterdf %>% 
  group_by(mean) %>% 
  summarize(mean(estimate)) %>% 
  rename(mu_hat_2 = `mean(estimate)`)

ggplot(plot2, aes(x = mean, y = mu_hat_2)) +
  geom_point() +
  geom_smooth(se = FALSE)

#plot 3
plot3 = masterdf %>% 
  filter(p.value < 0.05) %>% 
  group_by(mean) %>% 
  summarize(mean(estimate)) %>% 
  rename(mu_hat_2f = `mean(estimate)`)

ggplot(plot3, aes(x = mean, y = mu_hat_2f)) +
  geom_point() +
  geom_smooth(se = FALSE)

#the plot feels weird because there are - values. should i specify the data to be + only?

#how to overlap plot?

#-----
comp_plot = full_join(plot2, plot3)

#is the sample average of mu hat across tests for which the null is rejected approximately equal to the true value of mean? why or why not? 
#no. actually the data from mu_hat_2 has the closer value to the true mean compared to the filtered one. this is because they have bigger sample size (5000 vs 250 ish)

```



## Problem 3
```{r homicide}
#export the data
homicide = read.csv("./data/homicide-data.csv")
```

Describe the raw data: The data contains `r nrow(homicide)` observations. The variables are `r names(homicide)`, which includes date of the crime (in YYYYMMDD format), victim's name and demographics, location of homicide, and the outcome of the investigation. 

Create a city_state variable (e.g. “Baltimore, MD”) and then summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).
```{r homicide2}
homicide %>% 
  unite(city_state, c(city, state), sep = ", ") %>% 
  group_by(city_state) %>% 
  summarize(count = n())
```

*Am I supposed to merge homicide2 and homicide3? 

```{r homicide3}
homicide %>% 
  unite(city_state, c(city, state), sep = ", ") %>% 
  group_by(city_state, disposition) %>% 
  filter(disposition != "Closed by arrest") %>% 
  summarize(count = n())
  
```

merged homicide 2 and 3

```{r hom test}
homicide %>% 
  unite(city_state, c(city, state), sep = ", ") %>% 
  group_by(city_state) %>%
  summarize(
    total = n(), 
    unsolved = sum(disposition != "Closed by arrest"))
```

For the city of Baltimore, MD, use the prop.test function to estimate the proportion of homicides that are unsolved; save the output of prop.test as an R object, apply the broom::tidy to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.
```{r homicide4}
#prop test for baltimore. x = unsolved, n = total homicide, the rest use default setting
md_result = tidy(prop.test(1825, 2827)) %>% 
  mutate(city_state = "baltimore_md")
```

Now run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. Do this within a “tidy” pipeline, making use of purrr::map, purrr::map2, list columns and unnest as necessary to create a tidy dataframe with estimated proportions and CIs for each city.
```{r homicide5}
#now need to make a loop for function in homicide4 to look at all city state

#the function needs to be able to sample from the df, specifying each city state, x = unsolved, n = total homicide
#to sample by row, use rowwise() code

#where does citystate will be included in the prop.test function?

hom_result = homicide %>% 
  unite(city_state, c(city, state), sep = ", ") %>% 
  group_by(city_state) %>%
  summarize(
    total = n(), 
    unsolved = sum(disposition != "Closed by arrest")) %>%
  rowwise() %>%
  mutate(
    test_result = list(prop.test(x = unsolved, n = total)),
    p_value = test_result$p.value,
    estimate = test_result$estimate,
    conf_low = test_result$conf.int[1],
    conf_high = test_result$conf.int[2]
  ) %>%
  unnest(c(p_value, estimate, conf_low, conf_high))

#he wants to use purr so need to make a loop (which is???)

#write the function
homicide_loop = function(u) {
  hom_result = homicide %>% 
  unite(city_state, c(city, state), sep = ", ") %>% 
  group_by(city_state) %>%
  summarize(
    total = n(), 
    unsolved = sum(disposition != "Closed by arrest"))
}

hom_result2 = homicide %>%
  unite(city_state, c(city, state), sep = ", ") %>% 
  group_by(city_state) %>%
  summarize(
    total = n(), 
    unsolved = sum(disposition != "Closed by arrest")) %>% 
  map(hom_result2, prop.test(x = hom_result2$unsolved, n = hom_result2$total)) %>% 
  unnest(c(p_value, estimate, conf_low, conf_high))

# Extract the p-value, estimate, and confidence interval from the prop.test results
df <- df %>%
  mutate(
    p_value = map_dbl(test_result, ~ .x$p.value),
    estimate = map_dbl(test_result, ~ .x$estimate),
    conf_low = map_dbl(test_result, ~ .x$conf.int[1]),
    conf_high = map_dbl(test_result, ~ .x$conf.int[2])
  )
```


Create a plot that shows the estimates and CIs for each city – check out geom_errorbar for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides


